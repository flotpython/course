{
 "metadata": {
  "notebookname": "Web crawler",
  "signature": "sha256:a50a02feadaedc0e752a36ea15917da95e3c4bfa87202ce20bf8309f544d283a",
  "version": "1.0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Crawler Web "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dans ce projet, nous allons impl\u00e9menter un simple [crawler Web](http://fr.wikipedia.org/wiki/Robot_d%27indexation), c'est-\u00e0-dire un outil capable de parcourir des pages Web en suivant les URLs. C'est typiquement ce que font les moteurs de recherche comme Google. Notre objectif ici est de jouer avec certains de concepts importants que nous avons d\u00e9couvert dans ce MOOC et de pratiquer quelques librairies standards, mais nous ne chercherons pas la performance parce que \u00e7a augmenterait tr\u00e8s rapidement la complexit\u00e9 du code et la difficult\u00e9 du sujet. Cependant, vous constaterez que m\u00eame si ce crawler n'est pas adapt\u00e9 \u00e0 crawler des millions de pages, il est parfaitement capable de crawler des dizaines de milliers de pages et de vous rendre des services (comme identifier les liens morts sur un site Web)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "R\u00e9alisation du crawler Web"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ce projet est d\u00e9coup\u00e9 en deux niveaux de difficult\u00e9s. Nous allons commencer par le niveau avanc\u00e9 qui va vous demander d'\u00e9crire vous m\u00eame tout le code en fonction de nos sp\u00e9cifications de haut niveau. Pour le niveau interm\u00e9diaire, nous vous fournirons une description plus pr\u00e9cise de notre impl\u00e9mentation. \u00c0 vous de choisir o\u00f9 vous voulez commencer. \n",
      "\n",
      "Il est tr\u00e8s important de comprendre que le code que l'on vous propose d'\u00e9crire n'est ni totalement fiable, ni valid\u00e9 par des tests. Est-ce que cela est un probl\u00e8me pour ce projet ? Non, mais il est important dans du vrai code de production de le fiabiliser au maximum (en capturant les exceptions avec des reactions appropri\u00e9s et en testant toutes les entr\u00e9es) et de le tester (en ajoutant, par exemple, des tests unitaires et des tests fonctionnels). Cela a \u00e9videment un co\u00fbt tr\u00e8s \u00e9l\u00e9v\u00e9 en terme d'augmentation du code (on peut facilement multiplier par 2 ou 3 le nombre de lignes de code) et de temps de d\u00e9veloppement (il va falloir imaginer tous les cas \u00e0 tester).\n",
      "\n",
      "Le but de notre crawler est, \u00e0 partir de l'URL d'une page Web initiale, d'extraire tous les liens hypertexts des pages parcourues et d'utiliser ces liens pour parcourir de nouvelles pages et extraire de nouveaux liens. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Niveau avanc\u00e9"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Niveau interm\u00e9diaire"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tout notre programme peut-\u00eatre \u00e9crit dans un m\u00eame module. Nous avons dans ce module deux classes.\n",
      "\n",
      " * La classe `HTMLPage` repr\u00e9sente une page HTML. En particulier, une instance de cette classe a&nbsp;:\n",
      "  * un attribut `url` qui est une cha\u00eene de caract\u00e8res repr\u00e9sentant l'URL correspondant \u00e0 la page Web, \n",
      "  * un attribut `urls` qui est une liste de toutes les URLs trouv\u00e9es dans cette page,\n",
      "  * un attribut `http_code` qui est le code HTTP retourn\u00e9 par la requ\u00eate sur `url`.\n",
      "  \n",
      " Cette classe a trois m\u00e9thodes.\n",
      "  * Le constructeur prend comme argument une URL (sous forme d'une cha\u00eene de caract\u00e8res).\n",
      "  * La m\u00e9thode `page_fetcher` prend comme argument une URL et retourne un it\u00e9rateur sur la page HTML ou une liste vide en cas d'erreur. On utilisera la m\u00e9thode `urlopen` dans librairie standard `urllib2`. \n",
      "  * une m\u00e9thode `extract_urls_from_page` qui va parcourir l'it\u00e9rateur retourn\u00e9 par la m\u00e9thode `page_fetcher` et extraire toutes les URLs dans la page pour cr\u00e9er une liste de toutes les URLs dans la page, liste qui sera r\u00e9f\u00e9renc\u00e9e par l'attribut `urls` de l'instance. Pour extraire une URL, on cherchera dans le `body` de la page Web toutes les cha\u00eene de caract\u00e8res qui sont des valeurs de l'attribut `href` et qui commencent par `http` ou `https` (notons que c'est loin d'\u00eatre parfait). \n",
      "  \n",
      "  \n",
      "  \n",
      " * La classe `Crawler` permet de cr\u00e9er une instance d'un objet it\u00e9rable qui \u00e0 chaque it\u00e9ration retournera un nouvel objet HTMLPage qui a \u00e9t\u00e9 crawl\u00e9. L'instance du crawler va avoir comme attributs\n",
      "   * l'ensemble des sites \u00e0 crawler\n",
      "   * l'ensemble des sites d\u00e9j\u00e0 crawl\u00e9s\n",
      "   * un dictionnaire qui \u00e0 chaque URL fait correspondre la liste de tous les sites qui ont r\u00e9f\u00e9renc\u00e9s cette URL lors du crawl. \n",
      "   \n",
      " Cette classe a trois m\u00e9thodes.\n",
      "   * Le constructeur prend comme argument l'URL \u00e0 partir de laquelle on commence le crawl comme cha\u00eene de caract\u00e8res, le nombre maximum de sites \u00e0 crawler et une liste de domaines sur lesquels on veut restreindre le crawl. \n",
      "   * la m\u00e9thode update_sites_to_be_crawler qui prend comme argument un objet HTMLpage et qui met \u00e0 jour l'ensemble des sites \u00e0 crawler en fonction des URLs contenues dans la page pass\u00e9e en argument et des domaines sur lesquels on veut restreindre le crawl.\n",
      "   * une m\u00e9thode `__iter__` qui retourne un it\u00e9rateur qui \u00e0 chaque appel \u00e0 next() retourne un nouvel objet HTMLpage qui fait partie du crawl\n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ensuite, nous allons simplement cr\u00e9er un objet crawler et it\u00e9rer dessus pour extraire toutes les pages mortes (avec un code 404) que l'on trouve et tous les sites qui r\u00e9f\u00e9rences ces pages mortes. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Le mot de la fin"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous avons \u00e0 de nombreuses reprises \u00e9voqu\u00e9 la puissance de la librairie standard, mais aussi des librairies tierces. En particulier, nous avons insist\u00e9 sur le fait qu'au d\u00e9marrage d'un projet, il vaut mieux commencer par chercher si une librairie Python ne fait pas d\u00e9j\u00e0 tout ou partie de ce que vous voulez faire. \n",
      "\n",
      "Il existe une librairie Python tr\u00e8s puissante qui permet justement de faire des crawlers&nbsp;: il s'agit de [Scrapy](http://scrapy.org/). Maintenant que vous avez compris les bases d'un crawler Web, vous pourrez tirer pleinement b\u00e9n\u00e9fice des Scrapy. \n",
      "\n",
      "Il existe \u00e9galement un librairie pour parser du code HTML, c'est [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}